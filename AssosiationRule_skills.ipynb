{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79be83e0",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3252ff97-19b6-4833-b119-bdb5b36a545b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T17:47:10.725895Z",
     "start_time": "2023-04-15T17:47:10.463976Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25074fe",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb30ed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assignment_rel=pd.read_csv('./data/assignment_relationships.csv')\n",
    "df_training=pd.read_csv('./data/training_unit_test_scores.csv')\n",
    "df_actionlogs=pd.read_csv('./data/action_logs.csv')\n",
    "df_problem=pd.read_csv('./data/problem_details.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a6c54f",
   "metadata": {},
   "source": [
    "# Predict Score for in-unit problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00440d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select only required columns in data frames\n",
    "df_training=df_training.merge(df_assignment_rel,left_on='assignment_log_id',right_on='unit_test_assignment_log_id',how='left')\n",
    "df_training=df_training[['unit_test_assignment_log_id','problem_id', 'score', 'in_unit_assignment_log_id']]\n",
    "df_actionlogs=df_actionlogs[['assignment_log_id','problem_id','available_core_tutoring','action']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d78410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of action and available_core_tutoring features.\n",
    "one_hot = pd.get_dummies(df_actionlogs[['action','available_core_tutoring']])\n",
    "# Concatenate the original DataFrame with the one-hot encoded DataFrame\n",
    "df_actionlogs = pd.concat([df_actionlogs, one_hot], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf6d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actionlogs=df_actionlogs.rename(columns={'problem_id':'problem_id_action'})\n",
    "df_assosiation=df_training.merge(df_actionlogs,left_on='in_unit_assignment_log_id',right_on='assignment_log_id',how='left')\n",
    "df_actionlogs=df_actionlogs.dropna(subset = ['problem_id_action'])\n",
    "df_actionlogs_sub=df_actionlogs[['assignment_log_id','problem_id_action','action']]\n",
    "# getting the predicted score based on defination of scoring for end-of unit problems\n",
    "df_actionlogs_sub['predictedscore'] = df_actionlogs_sub.groupby(['assignment_log_id', 'problem_id_action'])['action'].transform(\n",
    "    lambda x: 1 if 'open_response' in x.tolist() \n",
    "    else (0 if any(action in ['wrong_response', 'hint_requested', 'explanation_requested','live_tutor_requested','skill_related_video_requested','answer_requested'] for action in x.tolist()) else None)\n",
    ")\n",
    "#only correct response is left in actions, so assigning score=1\n",
    "df_actionlogs_sub['predictedscore']=df_actionlogs_sub['predictedscore'].fillna(1)\n",
    "df_actionlogs_sub_temp=df_actionlogs_sub.drop(columns=['action'])\n",
    "df_actionlogs_sub_temp=df_actionlogs_sub_temp.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b59c4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get just required columns\n",
    "df_assosiation=df_assosiation[['unit_test_assignment_log_id', 'problem_id', 'score',\n",
    "       'in_unit_assignment_log_id', 'problem_id_action',]]\n",
    "#removing records where problem_id_action is null\n",
    "df_assosiation=df_assosiation.dropna(subset = ['problem_id_action'])\n",
    "df_assosiation=df_assosiation.rename(columns={'in_unit_assignment_log_id':'assignment_log_id'})\n",
    "df_assosiation_rule=df_assosiation.merge(df_actionlogs_sub_temp,on=['assignment_log_id','problem_id_action'],how='left')\n",
    "df_assosiation_rule=df_assosiation_rule.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371a44c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save predicted scores\n",
    "# df_assosiation_rule.to_pickle('/home/aswani/workspace/edm-cup/files/pickle_files/assosiationRule.pkl')\n",
    "# df_assosiation_rule=pd.read_pickle('/home/aswani/workspace/edm-cup/files/pickle_files/assosiationRule.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6983bfd4",
   "metadata": {},
   "source": [
    "# Make Transaction for assosiation rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef29602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'unit_test_assignment_log_id' and 'problem_id'\n",
    "groups = df_assosiation_rule.groupby(['unit_test_assignment_log_id', 'problem_id'])\n",
    "filtered_records = []\n",
    "\n",
    "# Iterate over each group\n",
    "for _, group_df in groups:\n",
    "    score = group_df['score'].iloc[0]\n",
    "    \n",
    "    # Filter records based on the 'score' and 'predictedscore' columns, \n",
    "    # we only consider the problem with either score 1 or score 0 both in in-unit and end of unit problems.\n",
    "    if score == 1:\n",
    "        filtered_group = group_df[group_df['predictedscore'] == 1]\n",
    "    elif score == 0:\n",
    "        filtered_group = group_df[group_df['predictedscore'] == 0]\n",
    "    else:\n",
    "        continue  # Skip the group if 'score' is neither 0 nor 1\n",
    "    \n",
    "    # Append the filtered records to the final result\n",
    "    filtered_records.append(filtered_group)\n",
    "\n",
    "# Combine the filtered records from each group\n",
    "filtered_df = pd.concat(filtered_records)\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "print(filtered_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e769533",
   "metadata": {},
   "source": [
    "### Get skill code for problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfdf2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get skill code for all problems in in-unit and end-of unit.\n",
    "filtered_df=filtered_df.merge(df_problem[['problem_id','problem_skill_code']],on='problem_id',how='left')\n",
    "filtered_df=filtered_df.merge(df_problem[['problem_id','problem_skill_code']],left_on='problem_id_action',right_on='problem_id',how='left')\n",
    "filtered_df=filtered_df.drop(columns=['problem_id_y'])\n",
    "filtered_df=filtered_df.rename(columns={'problem_skill_code_x':'problem_skill_code_train','problem_skill_code_y':'problem_skill_code_action'})\n",
    "filtered_df=filtered_df.rename(columns={'problem_skill_code_train':'problem_skill_code_train_full','problem_skill_code_action':'problem_skill_code_action_full'})\n",
    "filtered_df['problem_skill_code_train']=filtered_df['problem_skill_code_train_full'].str.split('.', n=3).str[0:2].str.join('.')\n",
    "filtered_df['problem_skill_code_action']=filtered_df['problem_skill_code_action_full'].str.split('.', n=3).str[0:2].str.join('.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cb81169",
   "metadata": {},
   "source": [
    "### Get the grade details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce6d636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first part of skill code is grade\n",
    "filtered_df['grade_train'] = filtered_df['problem_skill_code_train'].str.split('.', n=1).str[0]\n",
    "filtered_df['grade_action'] = filtered_df['problem_skill_code_action'].str.split('.', n=1).str[0]\n",
    "# we will divide the dataframes based on grade_action, so let's remove records with grade_action as NaN\n",
    "filtered_df=filtered_df.dropna(subset=['grade_train'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dc6b0b",
   "metadata": {},
   "source": [
    "### Barplot for grade vs average score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b76fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data frame to calculate average score per garde.\n",
    "filtered_df_fig=filtered_df.groupby('grade_train',as_index=False).agg({'score':'mean'})\n",
    "filtered_df_fig.score=filtered_df_fig.score.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a02507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,6),dpi=300)  # Increase the figure size\n",
    "# sns.barplot(data=filtered_df_fig,  x='grade_train',y='score',color='#7093cd')\n",
    "plt.bar(x=filtered_df_fig.grade_train, height=filtered_df_fig.score, width=0.5)\n",
    "plt.ylim(0.6)\n",
    "# plt.xticks(rotation=90)  # Rotate the x-axis labels by 90 degrees\n",
    "plt.xlabel('Grade', fontsize=18,fontweight='bold')\n",
    "plt.ylabel('Average Score', fontsize=18,fontweight='bold')\n",
    "plt.xticks(fontsize=14, fontweight='bold')\n",
    "plt.yticks(fontsize=14, fontweight='bold')\n",
    "plt.subplots_adjust(wspace=0.1) \n",
    "plt.savefig('Grade_Subject.pdf',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130fbbb1",
   "metadata": {},
   "source": [
    "### Separate data for score 0 & 1 and Get Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a021c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate dfs for score 1 and 0\n",
    "filtered_df_1=filtered_df[filtered_df['score']==1]\n",
    "filtered_df_0=filtered_df[filtered_df['score']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4a70c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group the DataFrame by the 'grade_train' column\n",
    "grouped1 = filtered_df_1.groupby('grade_train')\n",
    "# Create separate DataFrames for each group\n",
    "df_grades_1 = [group for _, group in grouped1]\n",
    "# Group the DataFrame by the 'grade_train' column\n",
    "grouped0 = filtered_df_0.groupby('grade_train')\n",
    "# Create separate DataFrames for each group\n",
    "df_grades_0 = [group for _, group in grouped0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c910cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with skill code\n",
    "# Function to prepare transactions \n",
    "# input: dataframe with scores for in-unit and end-of-unit problems and scores\n",
    "# Output: dataframe with new column skill_code_list, which will have transactions for each unit_test_assignment_log_id', 'problem_skill_code_train\n",
    "def group_and_extract_skill(filtered_df):\n",
    "    # Group the filtered data by 'unit_test_assignment_log_id' and 'problem_skill_code_train'\n",
    "    groups = filtered_df.groupby(['unit_test_assignment_log_id', 'problem_skill_code_train'])\n",
    "\n",
    "    result_records = []\n",
    "\n",
    "    # Iterate over each group\n",
    "    for _, group_df in groups:\n",
    "        unit_test_assignment_log_id = group_df['unit_test_assignment_log_id'].iloc[0]\n",
    "        skill_id = group_df['problem_skill_code_train'].iloc[0]\n",
    "\n",
    "        # Get the unique 'problem_skill_code_action' values and add 'skill_id' to the list\n",
    "        skill_code_list = group_df['problem_skill_code_action'].unique().tolist()\n",
    "        skill_code_list.append(skill_id)\n",
    "\n",
    "        # Create a dictionary with the required columns\n",
    "        result_dict = {\n",
    "            'unit_test_assignment_log_id': unit_test_assignment_log_id,\n",
    "            'problem_skill_code_train': skill_id,\n",
    "            'skill_code_list': skill_code_list\n",
    "        }\n",
    "\n",
    "        # Append the dictionary to the result records\n",
    "        result_records.append(result_dict)\n",
    "\n",
    "    # Create the final DataFrame\n",
    "    final_df = pd.DataFrame(result_records)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77da8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the group_and_extract function to each DataFrame and store them in a dictionary\n",
    "result_dfs_skill_1 = {}\n",
    "for i, df in enumerate(df_grades_1):\n",
    "    processed_df = group_and_extract_skill(df)\n",
    "    result_dfs_skill_1[f\"df_grades_1_{i+1}\"] = processed_df\n",
    "\n",
    "# Apply the group_and_extract function to each DataFrame and store them in a dictionary\n",
    "result_df_skill_0 = {}\n",
    "for i, df in enumerate(df_grades_0):\n",
    "    processed_df = group_and_extract_skill(df)\n",
    "    result_df_skill_0[f\"df_grades_0_{i+1}\"] = processed_df\n",
    "    \n",
    "#the result dictionaries result_dfs_skill_1,result_df_skill_0 have key as gradename and values as dataframes for each grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7056054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of transactions in each grade score 0\n",
    "row_counts = {}\n",
    "for key, value in result_df_skill_0.items():\n",
    "    row_counts[key] = value.shape[0]\n",
    "\n",
    "print(row_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e3b4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of transactions in each grade score 1\n",
    "row_counts = {}\n",
    "for key, value in result_dfs_skill_1.items():\n",
    "    row_counts[key] = value.shape[0]\n",
    "\n",
    "print(row_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5f4a4b1",
   "metadata": {},
   "source": [
    "# Frequent item sets and assosiation Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa5e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove null values in transactions\n",
    "# input: transaction lists\n",
    "def remove_nan(lst):\n",
    "    cleaned_lst = [[x for x in sublist if isinstance(x, str)] for sublist in lst]\n",
    "    return cleaned_lst\n",
    "\n",
    "problem_skill_code_training=list(filtered_df.problem_skill_code_train.unique())\n",
    "problem_skill_code_training=list(set(problem_skill_code_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392252c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#function to get frequent item sets and rules.\n",
    "def generate_association_rules(df, min_support, min_threshold):\n",
    "    # Initialize an empty list to store the rule dataframes\n",
    "    all_rules = []\n",
    "\n",
    "    # Iterate over the keys in the DataFrame\n",
    "    for key in df.keys():\n",
    "        # Get the transactions for the current key\n",
    "        transactions = df[key]['skill_code_list']\n",
    "        transactions_cleaned = remove_nan(transactions)\n",
    "        # Transform transactions into a binary encoded DataFrame\n",
    "        te = TransactionEncoder()\n",
    "        te_ary = te.fit(transactions_cleaned).transform(transactions_cleaned)\n",
    "        encoded_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "        # Find frequent itemsets\n",
    "        frequent_itemsets = fpgrowth(encoded_df, min_support=min_support, use_colnames=True)\n",
    "        # Check if frequent_itemsets is empty\n",
    "        if frequent_itemsets.empty:\n",
    "            continue  # Ignore and move to the next key\n",
    "        # Generate association rules\n",
    "        rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_threshold)\n",
    "        # Append the rules dataframe to the list\n",
    "        all_rules.append(rules)\n",
    "    # Concatenate all the rules dataframes into a single dataframe\n",
    "    concatenated_rules = pd.concat(all_rules, ignore_index=True)\n",
    "    concatenated_rules[\"consequents_len\"] = concatenated_rules[\"consequents\"].apply(lambda x: len(x))\n",
    "    concatenated_rules=concatenated_rules[concatenated_rules.consequents_len==1]\n",
    "    concatenated_rules[\"antecedents\"] = concatenated_rules[\"antecedents\"].apply(lambda x: ', '.join(list(x))).astype(\"unicode\")\n",
    "    concatenated_rules[\"consequents\"] = concatenated_rules[\"consequents\"].apply(lambda x: ', '.join(list(x))).astype(\"unicode\")\n",
    "    concatenated_rules=concatenated_rules.sort_values('confidence',ascending=False)\n",
    "    concatenated_rules['rule']=concatenated_rules['antecedents']+'-->'+concatenated_rules['consequents']\n",
    "    \n",
    "    return concatenated_rules\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b068e932",
   "metadata": {},
   "source": [
    "## Top rules for for score 0 with min support=0.7 and confidence=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6eae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score_0=generate_association_rules(result_df_skill_0, 0.05, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3286b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score_0[['rule', 'support', 'confidence']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5267980e",
   "metadata": {},
   "source": [
    "## Top rules for for score 1 with min support=0.8 and confidence=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd47170",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score_1=generate_association_rules(result_dfs_skill_1, 0.1, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97a2856",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score_1[['rule', 'support', 'confidence']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d974f91",
   "metadata": {},
   "source": [
    "## Rules for all grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57938636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function outputs assosiation rules for each grade seperately.\n",
    "#input: Dataframe which has skill codes as transactions\n",
    "#Output: assosiation Rules\n",
    "def generate_association_rules_grade(df, min_support, min_threshold):\n",
    "    # Get the transactions for the current DataFrame\n",
    "    transactions = df['skill_code_list']\n",
    "    transactions_cleaned = remove_nan(transactions)\n",
    "\n",
    "    # Transform transactions into a binary encoded DataFrame\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transactions_cleaned).transform(transactions_cleaned)\n",
    "    encoded_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "    # Find frequent itemsets\n",
    "    frequent_itemsets = fpgrowth(encoded_df, min_support=min_support, use_colnames=True)\n",
    "\n",
    "    # Check if frequent_itemsets is empty\n",
    "    if frequent_itemsets.empty:\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no frequent itemsets are found\n",
    "\n",
    "    # Generate association rules\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_threshold)\n",
    "\n",
    "    # Preprocess and format the rules DataFrame\n",
    "    rules[\"consequents_len\"] = rules[\"consequents\"].apply(lambda x: len(x))\n",
    "    rules = rules[rules.consequents_len == 1]\n",
    "    rules[\"antecedents\"] = rules[\"antecedents\"].apply(lambda x: ', '.join(list(x))).astype(\"unicode\")\n",
    "    rules[\"consequents\"] = rules[\"consequents\"].apply(lambda x: ', '.join(list(x))).astype(\"unicode\")\n",
    "    rules = rules.sort_values('confidence', ascending=False)\n",
    "    rules['rule'] = rules['antecedents'] + '-->' + rules['consequents']\n",
    "\n",
    "    return rules[['rule','support','confidence']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aab7c17",
   "metadata": {},
   "source": [
    "### Rules for score 0 -grade wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8d3077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print rules seperated by grade for score 0\n",
    "#Even though the rules has other skills than specified grade skills, all the consequents are end-of unit problem skills. so they are still valid.\n",
    "for key in result_df_skill_0.keys():\n",
    "    rules_df = generate_association_rules_grade(result_df_skill_0[key], 0.05, 0.5)\n",
    "    if not rules_df.empty:\n",
    "        print(f\"Association rules for DataFrame '{key}':\")\n",
    "        print(rules_df)\n",
    "        print(\"--------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78aa3ed",
   "metadata": {},
   "source": [
    "#### Top rule from each grade based on support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2a84df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print rules dataframe only with top one rule from each grade for score 0\n",
    "rule_table = pd.DataFrame(columns=['Rule', 'Support', 'Confidence'])\n",
    "for key in result_df_skill_0.keys():\n",
    "    rules_df = generate_association_rules_grade(result_df_skill_0[key], 0.1, 0.5)\n",
    "    if not rules_df.empty:\n",
    "        max_support_row = rules_df.nlargest(1, 'support').iloc[0]\n",
    "        rule = max_support_row['rule']\n",
    "        support = max_support_row['support']\n",
    "        confidence = max_support_row['confidence']\n",
    "        rule_table = rule_table.append({'Rule': rule, 'Support': support, 'Confidence': confidence}, ignore_index=True)\n",
    "\n",
    "print(rule_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39acb0da",
   "metadata": {},
   "source": [
    "### Rules for score 1 -grade wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f27894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print rules seperated by grade for score 1\n",
    "for key in result_dfs_skill_1.keys():\n",
    "    rules_df = generate_association_rules_grade(result_dfs_skill_1[key], 0.1, 0.5)\n",
    "    if not rules_df.empty:\n",
    "        print(f\"Association rules for DataFrame '{key}':\")\n",
    "        print(rules_df)\n",
    "        print(\"--------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10042d10",
   "metadata": {},
   "source": [
    "#### Top rule from each grade based on support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f853d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print rules dataframe only with top one rule from each grade for score 1\n",
    "rule_table = pd.DataFrame(columns=['Rule', 'Support', 'Confidence'])\n",
    "\n",
    "for key in result_dfs_skill_1.keys():\n",
    "    rules_df = generate_association_rules_grade(result_dfs_skill_1[key], 0.1, 0.5)\n",
    "    if not rules_df.empty:\n",
    "        max_support_row = rules_df.nlargest(1, 'support').iloc[0]\n",
    "        rule = max_support_row['rule']\n",
    "        support = max_support_row['support']\n",
    "        confidence = max_support_row['confidence']\n",
    "        rule_table = rule_table.append({'Rule': rule, 'Support': support, 'Confidence': confidence}, ignore_index=True)\n",
    "\n",
    "print(rule_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218bb2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
